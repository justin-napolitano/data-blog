{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/ai-proof-of-concept/",
    "result": {"data":{"site":{"siteMetadata":{"title":"Justin's Data Blog"}},"markdownRemark":{"id":"cd5fdd74-af2f-548e-877d-8d65a5ba7fa4","excerpt":"Conduct Legal Research with AI: Part 3 This is the third post in a series documenting the process of building an ml pipeline that will be used to train models…","html":"<h1>Conduct Legal Research with AI: Part 3</h1>\n<p>This is the third post in a series documenting the process of building an ml pipeline that will be used to train models to predict the outcomes of Supreme Court Cases.</p>\n<p>You can find the others at:</p>\n<ul>\n<li><a href=\"https://blog.jnapolitano.io/neo4j_integration/\">blog.jnapolitano.io/neo4j_integration/</a></li>\n<li><a href=\"https://blog.jnapolitano.io/constitution_to_neo/\">blog.jnapolitano.io/constitution_to_neo/</a></li>\n</ul>\n<h2>Introduction</h2>\n<p>In this post, I will be testing a sample TensorFlow pipeline against the <a href=\"http://scdb.wustl.edu/\">Supreme Court Database</a> maintained by the Washington University Law School in order to build a proof of concept model for a Supreme Court Graph Analysis project.</p>\n<h2>The Data Set</h2>\n<p>The data set is fairly limited. There are only about 250 issue areas to train against. It also does not include doctrinal trees to train an algorithm to predict when the court will break from precedent.</p>\n<p>The data records primarily the voting behavior of justices, the policy direction of the outcome, and the political ideology score of the outcome.</p>\n<p>Policy direction and political ideology are factors coded by individuals that may be prone to error an bias. In my analysis I will test against the findings of this model to determine which factors are most powerful.</p>\n<h2>The Model</h2>\n<p>I tested against case centric and justice centric data orientations.</p>\n<h3>Case Centric Model</h3>\n<p>The first is case centric.  The power of this model is fairly limited, as it attempts to discern the outcome of a case from the aggregate behavior of the current justices on the bench.  It essentially creates a score that is modified by each case outcome to determine if the court will vote conservatively or liberally towards a case of regarding a specific issue area.  It is important to note that the database limits each case to one overarching issue area.</p>\n<h4>Case Centric Training Features</h4>\n<ul>\n<li>issue</li>\n<li>issueArea</li>\n<li>naturalCourt</li>\n<li>petitioner</li>\n<li>petitionerState</li>\n<li>respondent</li>\n<li>jurisdiction</li>\n<li>adminAction</li>\n<li>adminActionState</li>\n<li>threeJudgeFdc</li>\n<li>caseOrigin</li>\n<li>caseOriginState</li>\n<li>respondentState</li>\n<li>caseSource</li>\n<li>caseSourceState</li>\n<li>lawType</li>\n<li>lawSupp</li>\n</ul>\n<h4>Case Centric Results</h4>\n<p>The models were able to predict the outcomes about 75 percent of the time following 1,000 epochs of training.  It is likely that factors other than previous voting behavior determine the outcome of the case.</p>\n<h3>Justice Centric Model</h3>\n<p>The second is justice centric.  The predictive power of this model, should in theory be greater as it attempts to determine the likely vote of an individual justice against an issue area.  Again this is limited as only the overarching issue area is available in the data.  It may very well prove that justices may favor free speech in most cases, except when the military is the target.  The model could not predict that, because the data is limited.</p>\n<h4>Justice Centric Training Features</h4>\n<ul>\n<li>justice</li>\n<li>issue</li>\n<li>issueArea</li>\n<li>lawType</li>\n<li>direction</li>\n<li>petitioner</li>\n<li>naturalCourt</li>\n<li>petitionerState</li>\n<li>respondent</li>\n<li>respondentState</li>\n<li>jurisdiction</li>\n<li>caseOrigin</li>\n<li>caseOriginState</li>\n<li>caseSource</li>\n<li>caseSourceState</li>\n<li>certReason</li>\n<li>lawSupp</li>\n</ul>\n<h4>Justice Centric Results</h4>\n<p>The justice centric pipeline results in accuracy exceeding 82 percent.  This is better, but a model that could consider more factors and issue areas would likely improve the results.</p>\n<h2>Next Steps</h2>\n<p>I will integrate the models to my neo4j data set.  As of now, the graph database exceeds 50,000 individuals nodes and over 400,000 relationships.  It will allow me to train an algorithm that can determine a justices attitude towards, cases, clauses, sentences, states, petitioners, and any other feature integrated into the dataset.</p>\n<h2>The Code</h2>\n<h3>Case Centric Training Pipeline</h3>\n<p>I’ve included the entire model below.  Review my <a href=\"https://docs.jnapolitano.io/parts/ml-ai/tensorflow/project-supcourt-tensorflow/docs/SupremeCourtPredictionsCase/index.html\">documentation site</a> for more detail.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">#supremeCourtPrediction.py\n#a program that uses the segal and spaeth dataset to predict the outcomes of cases\n\n\nimport pandas as pd\n\nimport tensorflow as tf\n\nimport math\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport shap\nshap.initjs()\n\nimport os \nimport csv\nimport datetime\n\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\n\nfrom itertools import permutations \n\nimport matplotlib.pyplot as plt\n\n\n\ndef get_environmental_variables():\n    cwd = os.getcwd()\n    log_dir = os.path.join(cwd,\"logs/fit/\") + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    return cwd, log_dir\n\n\n\ndef load_data(cwd):\n    data_path = os.path.join(cwd,'data','citation.csv') \n    drop_path = os.path.join(cwd,'data','features','drop','drop.txt')\n    #print(path)\n    dataframe = pd.read_csv(data_path,encoding= 'unicode_escape')\n    #print(dataframe.head())\n    \n    with open(drop_path) as fp:\n        for cnt, line in enumerate(fp):\n            #print(\"Dropping {} from dataframe\".format(line))\n            line = line.strip('\\n')\n            line = line.strip('\\t')\n            line = line.strip(\"'\")\n\n            dataframe.drop(line,axis=1,inplace=True)\n    \n    dataframe.fillna(0, inplace=True)\n    print(dataframe)\n    #dataframe.issue = dataframe.astype({'issue': 'str'})\n    #dataframe[issue] = dataframe[issue].astype(str)\n    #print(dataframe)\n    #print(dataframe.head())\n    return dataframe\n\n\n\ndef split_datframe(dataframe):\n    train, test = train_test_split(dataframe, test_size=0.2)\n    train, val = train_test_split(train, test_size=0.2)\n    print(len(train), 'train examples')\n    print(len(val), 'validation examples')\n    print(len(test), 'test examples')\n    return train, test, val\n\n\n    # A utility method to create a tf.data dataset from a Pandas Dataframe\n\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop('declarationUncon')\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    print(ds)\n    return ds\n\n\n\ndef get_input_pipeline(train, test, val, batch_size=32, shuffle=True):\n    train_ds = df_to_dataset(train, batch_size=batch_size)\n    val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n    test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n    return train_ds, val_ds, test_ds\n\n\n\ndef get_feature_layer(cwd): #select the columns for analysis from dataset\n    feature_path = os.path.join(cwd,'data','features','use','features.txt')\n    dict_path = os.path.join(cwd,'data','features','use')\n    feature_columns = []\n    \n    #embedded columns\n    with open(feature_path) as fp:\n        for cnt, line in enumerate(fp):\n            #print(\"Adding {} to features\".format(line))\n            line = line.strip('\\n')\n            line = line.strip('\\t')\n            line = line.strip(\"'\")\n\n            print(cnt)   \n            print(line)\n\n            indicator = feature_column.numeric_column(line)\n            \n            #feature_column.categorical_column_with_vocabulary_file(\n            #            key = line, vocabulary_file = os.path.join(dict_path,\"{}.txt\".format(line)), default_value=0)\n\n            print(indicator)\n            feature_columns.append(indicator)\n\n\n            #feature_columns.append(feature_column.embedding_column(indicator, dimension=8))\n\n    \n    crossed_feature = feature_column.crossed_column(['issue', 'issueArea', 'naturalCourt'], hash_bucket_size=1000)\n    crossed_feature = feature_column.indicator_column(crossed_feature)\n    feature_columns.append(crossed_feature)\n\n\n    crossed_feature = feature_column.crossed_column(['issue', 'naturalCourt'], hash_bucket_size=1000)\n    crossed_feature = feature_column.indicator_column(crossed_feature)\n    feature_columns.append(crossed_feature)\n\n    crossed_feature = feature_column.crossed_column(['issueArea', 'naturalCourt'],hash_bucket_size=1000)\n    crossed_feature = feature_column.indicator_column(crossed_feature)\n    feature_columns.append(crossed_feature)\n    \n   # court_buckets = feature_column.bucketized_column(naturalCourt, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n\n    #print(feature_columns)\n    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n    #for item in feature_columns:\n    #    print(item)\n    #    print(\"\\n\")\n\n    return feature_layer\n\n     \n    #crossed cols\n\n\n    #categorical_columns \n\ndef understand_input_pipeline(train_ds):\n    for feature_batch, label_batch in train_ds.take(1):\n        print('Every feature:', list(feature_batch.keys()))\n        #print('A batch of ages:', feature_batch['age'])\n        print('A batch of targets:', label_batch )\n\n\n\ndef create_model(log_dir,feature_layer, train_ds, val_ds, epochs = 5):\n    \n    model = tf.keras.Sequential([\n        feature_layer,\n        layers.Dense(128, activation='relu'),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(1)\n    ])\n\n    model.compile(optimizer='adam',\n            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n            metrics=['accuracy', 'mae'])\n\n    history = model.fit(train_ds,\n            validation_data=val_ds,\n            epochs=epochs)\n\n    loss, accuracy,mae = model.evaluate(test_ds)\n    print(\"Accuracy\", accuracy)\n    print(model.summary())\n    return model,history\n\n\ndef plot_history(history):\n\n    hist = pd.DataFrame(history.history)\n    print(hist)\n    hist['epoch'] = history.epoch\n\n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean Absolute Error')\n    plt.plot(hist['epoch'], hist['mae'],\n            label='Train Error')\n    plt.plot(hist['epoch'], hist['val_mae'],\n            label = 'Val Error')\n    plt.legend()\n\n\n\ndef explain_kernal(df_train,model,train_ds):\n    \n    # KernelExplainer is a general approach that can work with any ML framework\n    # Its inputs are the predictions and training data\n\n    # Summarize the training set to accelerate analysis\n    df_train_summary = shap.kmeans(df_train.values, 25)\n\n    # Instantiate an explainer with the model predictions and training data summary\n    explainer = shap.KernelExplainer(model.predict, df_train)\n\n\n# Extract Shapley values from the explainer\n    #shap_values = explainer.shap_values(df_train.values)\nif __name__ == \"__main__\":\n    cwd, log_dir = get_environmental_variables()\n    #print(cwd)\n\n    df = load_data(cwd)\n\n    train,test,val = split_datframe(df)\n    dataset = df_to_dataset(df)\n    train_ds, val_ds, test_ds = get_input_pipeline(train,test,val,32)\n    understand_input_pipeline(train_ds)\n    feature_layer = get_feature_layer(cwd)\n    model, model_history = create_model(log_dir,feature_layer, train_ds, val_ds)\n    plot_history(model_history)\n    #print(model.predict(train_ds))\n    #print(model.predict)\n    explain_kernal(df,model,train_ds)</code></pre></div>\n<h3>The Justice Centric Training Pipeline</h3>\n<p>The entire pipeline is included below.  Review the <a href=\"https://docs.jnapolitano.io/parts/ml-ai/tensorflow/project-supcourt-tensorflow/docs/SupremeCourtPredictionsJustice/index.html\">documentation</a> for greater detail.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">#supremeCourtPrediction.py\n#a program that uses the segal and spaeth dataset to predict the outcomes of cases\n\n\nimport pandas as pd\n\nimport tensorflow as tf\n\nimport math\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport shap\nshap.initjs()\n\nimport os \nimport csv\nimport datetime\n\nfrom tensorflow import feature_column\nfrom tensorflow.keras import layers\nfrom sklearn.model_selection import train_test_split\n\nfrom itertools import permutations\nfrom itertools import combinations_with_replacement\n\nimport matplotlib.pyplot as plt\n\n\n\n\ndef get_environmental_variables():\n    cwd = os.getcwd()\n    log_dir = os.path.join(cwd,\"logs/fit/\") + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    return cwd, log_dir\n\n\n\n\ndef load_data(cwd):\n    data_path = os.path.join(cwd,'data','justice.csv') \n    drop_path = os.path.join(cwd,'data','features','drop','drop.txt')\n    target_path = os.path.join(cwd,'data','features','targets.txt')\n    print(data_path)\n    dataframe = pd.read_csv(data_path,encoding= 'unicode_escape')\n    #print(dataframe.head())\n    \n    with open(drop_path) as fp:\n        for cnt, line in enumerate(fp):\n            #print(\"Dropping {} from dataframe\".format(line))\n            line = line.strip('\\n')\n            line = line.strip('\\t')\n            line = line.strip(\"'\")\n\n            dataframe.drop(line,axis=1,inplace=True)\n    \n    dataframe.fillna(0, inplace=True)\n    print(dataframe)\n    #dataframe.issue = dataframe.astype({'issue': 'str'})\n    #dataframe[issue] = dataframe[issue].astype(str)\n    #print(dataframe)\n    #print(dataframe.head())\n    return dataframe\n\n\n\n\ndef split_datframe(dataframe):\n    train, test = train_test_split(dataframe, test_size=0.2)\n    train, val = train_test_split(train, test_size=0.2)\n    print(len(train), 'train examples')\n    print(len(val), 'validation examples')\n    print(len(test), 'test examples')\n    return train, test, val\n\n\n    # A utility method to create a tf.data dataset from a Pandas Dataframe\n\n\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop('vote')\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    print(ds)\n    return ds\n\n\n\n\ndef get_input_pipeline(train, test, val, batch_size=32, shuffle=True):\n    train_ds = df_to_dataset(train, batch_size=batch_size)\n    val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n    test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n    return train_ds, val_ds, test_ds\n\n\n\n\ndef get_feature_layer(cwd): #select the columns for analysis from dataset\n    feature_path = os.path.join(cwd,'data','features','use','features.txt')\n    dict_path = os.path.join(cwd,'data','features','use')\n    feature_columns = []\n    feature_list = []\n    #embedded columns\n    with open(feature_path) as fp:\n        for cnt, line in enumerate(fp):\n            #print(\"Adding {} to features\".format(line))\n            line = line.strip('\\n')\n            line = line.strip('\\t')\n            line = line.strip(\"'\")\n\n            print(cnt)   \n            print(line)\n            feature_list.append(line)\n\n            indicator = feature_column.numeric_column(line)\n            \n            #feature_column.categorical_column_with_vocabulary_file(\n            #            key = line, vocabulary_file = os.path.join(dict_path,\"{}.txt\".format(line)), default_value=0)\n\n            print(indicator)\n            feature_columns.append(indicator)\n\n\n            #feature_columns.append(feature_column.embedding_column(indicator, dimension=8))\n\n    feature_comb = combinations_with_replacement(feature_list, 2) \n    lst = list(feature_comb)\n    limit = len(feature_list)\n    print(limit)\n    j = 0\n    k = limit - 1\n    lookup = []\n    for i in range(limit):\n        lookup.append(i+j)\n        j = j + k\n        k = k - 1 \n\n\n    for i in range(len(lst)):\n        if i in lookup:\n            continue\n        else:\n            one, two = lst[i]\n            crossed_feature = feature_column.crossed_column([one, two], hash_bucket_size=1000)\n            crossed_feature = feature_column.indicator_column(crossed_feature)\n            feature_columns.append(crossed_feature)\n\n    \"\"\"\n    crossed_feature = feature_column.crossed_column(['issue', 'naturalCourt'], hash_bucket_size=1000)\n    crossed_feature = feature_column.indicator_column(crossed_feature)\n    feature_columns.append(crossed_feature)\n\n    crossed_feature = feature_column.crossed_column(['issueArea', 'naturalCourt'],hash_bucket_size=1000)\n    crossed_feature = feature_column.indicator_column(crossed_feature)\n    feature_columns.append(crossed_feature)\n    \"\"\"\n   # court_buckets = feature_column.bucketized_column(naturalCourt, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n\n    #print(feature_columns)\n    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n    #for item in feature_columns:\n    #    print(item)\n    #    print(\"\\n\")\n\n    return feature_layer\n\n     \n    #crossed cols\n\n\n    #categorical_columns \n\n\ndef understand_input_pipeline(train_ds):\n    for feature_batch, label_batch in train_ds.take(1):\n        print('Every feature:', list(feature_batch.keys()))\n        #print('A batch of ages:', feature_batch['age'])\n        print('A batch of targets:', label_batch )\n\n\n\n\ndef create_model(log_dir,feature_layer, train_ds, val_ds, epochs = 8):\n    \n    model = tf.keras.Sequential([\n        feature_layer,\n        layers.Dense(128, activation='relu'),\n        layers.Dense(128, activation='relu'),\n        layers.Dense(1)\n    ])\n\n    model.compile(optimizer='adam',\n            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n            metrics=['accuracy', 'mae'])\n\n    history = model.fit(train_ds,\n            validation_data=val_ds,\n            epochs=epochs)\n\n    loss, accuracy,mae = model.evaluate(test_ds)\n    print(\"Accuracy\", accuracy)\n    print(model.summary())\n    return model,history\n\n\n\n\ndef plot_history(history):\n\n    hist = pd.DataFrame(history.history)\n    print(hist)\n    hist['epoch'] = history.epoch\n\n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean Absolute Error')\n    plt.plot(hist['epoch'], hist['mae'],\n            label='Train Error')\n    plt.plot(hist['epoch'], hist['val_mae'],\n            label = 'Val Error')\n    plt.legend()\n\n\n\n\ndef explain_kernal(model,train_ds):\n    model.predict(train_ds)\n    # KernelExplainer is a general approach that can work with any ML framework\n    # Its inputs are the predictions and training data\n\n    # Summarize the training set to accelerate analysis\n    df_train_summary = shap.kmeans(train_ds.values, 25)\n    #print(df_train_summary)\n    #model.predict(df_train_summary)\n    # Instantiate an explainer with the model predictions and training data summary\n    explainer = shap.KernelExplainer(model.predict, train_ds)\n\n\n# Extract Shapley values from the explainer\n    #shap_values = explainer.shap_values(df_train.values)\nif __name__ == \"__main__\":\n    cwd, log_dir = get_environmental_variables()\n    print(cwd)\n\n\n    df = load_data(cwd)\n\n    train,test,val = split_datframe(df)\n    dataset = df_to_dataset(df)\n    train_ds, val_ds, test_ds = get_input_pipeline(train,test,val,32)\n    understand_input_pipeline(train_ds)\n    feature_layer = get_feature_layer(cwd)\n    model, model_history = create_model(log_dir,feature_layer, train_ds, val_ds)\n    #plot_history(model_history)\n    #print(model.predict(train_ds))\n    #print(model.predict)\n    #explain_kernal(model,train_ds)</code></pre></div>","frontmatter":{"title":"Conduct Legal Research with AI: Part 3","date":"May 17, 2022","description":"Predicting the outcomes of Supreme Court Cases with TensorFlow","imageAlt":"Justin Napolitano","author":"Justin Napolitano","image":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+0lEQVQoz2P4jwO8f/X13bOPv///e/H5Ay41DJhC//7+O77k5MyotsWxBVvbmzIb8lcc3UGs5r9//i0rXDbLyH6uutpMVfVaV7ukyTU///wGmUtQ859ff1fWrp5ubLdET2+GlnaDi33a5Pofv38Rpfnf33+fXn862NaxyMRogrHJ8Wkzbj29//P3b8LO/vcPZPr3Hz/jE1tcdD0sdJxKSycTG2AQze/efjQ1DtTSiFTTdLa3D/327SdcirDmt28+GBp5aKl7q6gbO7mE/PjxmwTNP77/io8rkZezV1E3S04u/f37L1iKiKiCgF8/f29Ye+DQwTO/fv3G5WcAwE4LnUVWCOEAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/515df4a41de5943f02d20899cf6a3b71/a6312/post-image.png","srcSet":"/static/515df4a41de5943f02d20899cf6a3b71/e7dcc/post-image.png 750w,\n/static/515df4a41de5943f02d20899cf6a3b71/50eb2/post-image.png 1080w,\n/static/515df4a41de5943f02d20899cf6a3b71/a6312/post-image.png 1200w","sizes":"100vw"},"sources":[{"srcSet":"/static/515df4a41de5943f02d20899cf6a3b71/06049/post-image.avif 750w,\n/static/515df4a41de5943f02d20899cf6a3b71/0f115/post-image.avif 1080w,\n/static/515df4a41de5943f02d20899cf6a3b71/5b80a/post-image.avif 1200w","type":"image/avif","sizes":"100vw"},{"srcSet":"/static/515df4a41de5943f02d20899cf6a3b71/ee7ce/post-image.webp 750w,\n/static/515df4a41de5943f02d20899cf6a3b71/819dc/post-image.webp 1080w,\n/static/515df4a41de5943f02d20899cf6a3b71/f9756/post-image.webp 1200w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.5}}}}},"previous":{"fields":{"slug":"/constitution_to_neo/"},"frontmatter":{"title":"Conduct Legal Research with AI: Part 2"}},"next":{"fields":{"slug":"/legal-research-part-4/"},"frontmatter":{"title":"Conduct Legal Research with AI: Part 4"}}},"pageContext":{"id":"cd5fdd74-af2f-548e-877d-8d65a5ba7fa4","previousPostId":"6392a68f-8513-540d-aeec-9993aeb06092","nextPostId":"7af77a27-3ab1-5589-b476-19df0feceb3b"}},
    "staticQueryHashes": ["2841359383","3257411868"]}