<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="x-ua-compatible" content="ie=edge"/><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"/><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"/><meta data-react-helmet="true" name="twitter:image:alt" content="Justin Napolitano"/><meta data-react-helmet="true" name="twitter:image" content="undefined/static/515df4a41de5943f02d20899cf6a3b71/a6312/post-image.png"/><meta data-react-helmet="true" name="og:image:alt" content="Justin Napolitano"/><meta data-react-helmet="true" name="og:image" content="undefined/static/515df4a41de5943f02d20899cf6a3b71/a6312/post-image.png"/><meta data-react-helmet="true" name="twitter:description" content="Predicting the outcomes of Supreme Court Cases with TensorFlow"/><meta data-react-helmet="true" name="twitter:title" content="Conduct Legal Research with AI: Part 3"/><meta data-react-helmet="true" name="twitter:creator" content="just_napolitano"/><meta data-react-helmet="true" name="twitter:card" content="summary"/><meta data-react-helmet="true" property="og:type" content="website"/><meta data-react-helmet="true" property="og:description" content="Predicting the outcomes of Supreme Court Cases with TensorFlow"/><meta data-react-helmet="true" property="og:title" content="Conduct Legal Research with AI: Part 3"/><meta data-react-helmet="true" name="description" content="Predicting the outcomes of Supreme Court Cases with TensorFlow"/><meta name="generator" content="Gatsby 4.12.1"/><style data-href="/styles.5ed63f97d76756aacddc.css" data-identity="gatsby-global-css">@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:100;src:local("Montserrat Thin "),local("Montserrat-Thin"),url(/static/montserrat-latin-100-8d7d79679b70dbe27172b6460e7a7910.woff2) format("woff2"),url(/static/montserrat-latin-100-ec38980a9e0119a379e2a9b3dbb1901a.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:100;src:local("Montserrat Thin italic"),local("Montserrat-Thinitalic"),url(/static/montserrat-latin-100italic-e279051046ba1286706adc886cf1c96b.woff2) format("woff2"),url(/static/montserrat-latin-100italic-3b325a3173c8207435cd1b76e19bf501.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:200;src:local("Montserrat Extra Light "),local("Montserrat-Extra Light"),url(/static/montserrat-latin-200-9d266fbbfa6cab7009bd56003b1eeb67.woff2) format("woff2"),url(/static/montserrat-latin-200-2d8ba08717110d27122e54c34b8a5798.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:200;src:local("Montserrat Extra Light italic"),local("Montserrat-Extra Lightitalic"),url(/static/montserrat-latin-200italic-6e5b3756583bb2263eb062eae992735e.woff2) format("woff2"),url(/static/montserrat-latin-200italic-a0d6f343e4b536c582926255367a57da.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:300;src:local("Montserrat Light "),local("Montserrat-Light"),url(/static/montserrat-latin-300-00b3e893aab5a8fd632d6342eb72551a.woff2) format("woff2"),url(/static/montserrat-latin-300-ea303695ceab35f17e7d062f30e0173b.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:300;src:local("Montserrat Light italic"),local("Montserrat-Lightitalic"),url(/static/montserrat-latin-300italic-56f34ea368f6aedf89583d444bbcb227.woff2) format("woff2"),url(/static/montserrat-latin-300italic-54b0bf2c8c4c12ffafd803be2466a790.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:400;src:local("Montserrat Regular "),local("Montserrat-Regular"),url(/static/montserrat-latin-400-b71748ae4f80ec8c014def4c5fa8688b.woff2) format("woff2"),url(/static/montserrat-latin-400-0659a9f4e90db5cf51b50d005bff1e41.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:400;src:local("Montserrat Regular italic"),local("Montserrat-Regularitalic"),url(/static/montserrat-latin-400italic-6eed6b4cbb809c6efc7aa7ddad6dbe3e.woff2) format("woff2"),url(/static/montserrat-latin-400italic-7583622cfde30ae49086d18447ab28e7.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:500;src:local("Montserrat Medium "),local("Montserrat-Medium"),url(/static/montserrat-latin-500-091b209546e16313fd4f4fc36090c757.woff2) format("woff2"),url(/static/montserrat-latin-500-edd311588712a96bbf435fad264fff62.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:500;src:local("Montserrat Medium italic"),local("Montserrat-Mediumitalic"),url(/static/montserrat-latin-500italic-c90ced68b46050061d1a41842d6dfb43.woff2) format("woff2"),url(/static/montserrat-latin-500italic-5146cbfe02b1deea5dffea27a5f2f998.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:600;src:local("Montserrat SemiBold "),local("Montserrat-SemiBold"),url(/static/montserrat-latin-600-0480d2f8a71f38db8633b84d8722e0c2.woff2) format("woff2"),url(/static/montserrat-latin-600-b77863a375260a05dd13f86a1cee598f.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:600;src:local("Montserrat SemiBold italic"),local("Montserrat-SemiBolditalic"),url(/static/montserrat-latin-600italic-cf46ffb11f3a60d7df0567f8851a1d00.woff2) format("woff2"),url(/static/montserrat-latin-600italic-c4fcfeeb057724724097167e57bd7801.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:700;src:local("Montserrat Bold "),local("Montserrat-Bold"),url(/static/montserrat-latin-700-7dbcc8a5ea2289d83f657c25b4be6193.woff2) format("woff2"),url(/static/montserrat-latin-700-99271a835e1cae8c76ef8bba99a8cc4e.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:700;src:local("Montserrat Bold italic"),local("Montserrat-Bolditalic"),url(/static/montserrat-latin-700italic-c41ad6bdb4bd504a843d546d0a47958d.woff2) format("woff2"),url(/static/montserrat-latin-700italic-6779372f04095051c62ed36bc1dcc142.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:800;src:local("Montserrat ExtraBold "),local("Montserrat-ExtraBold"),url(/static/montserrat-latin-800-db9a3e0ba7eaea32e5f55328ace6cf23.woff2) format("woff2"),url(/static/montserrat-latin-800-4e3c615967a2360f5db87d2f0fd2456f.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:800;src:local("Montserrat ExtraBold italic"),local("Montserrat-ExtraBolditalic"),url(/static/montserrat-latin-800italic-bf45bfa14805969eda318973947bc42b.woff2) format("woff2"),url(/static/montserrat-latin-800italic-fe82abb0bcede51bf724254878e0c374.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:normal;font-weight:900;src:local("Montserrat Black "),local("Montserrat-Black"),url(/static/montserrat-latin-900-e66c7edc609e24bacbb705175669d814.woff2) format("woff2"),url(/static/montserrat-latin-900-8211f418baeb8ec880b80ba3c682f957.woff) format("woff")}@font-face{font-display:swap;font-family:Montserrat;font-style:italic;font-weight:900;src:local("Montserrat Black italic"),local("Montserrat-Blackitalic"),url(/static/montserrat-latin-900italic-4454c775e48152c1a72510ceed3603e2.woff2) format("woff2"),url(/static/montserrat-latin-900italic-efcaa0f6a82ee0640b83a0916e6e8d68.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:normal;font-weight:300;src:local("Merriweather Light "),local("Merriweather-Light"),url(/static/merriweather-latin-300-fc117160c69a8ea0851b26dd14748ee4.woff2) format("woff2"),url(/static/merriweather-latin-300-58b18067ebbd21fda77b67e73c241d3b.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:italic;font-weight:300;src:local("Merriweather Light italic"),local("Merriweather-Lightitalic"),url(/static/merriweather-latin-300italic-fe29961474f8dbf77c0aa7b9a629e4bc.woff2) format("woff2"),url(/static/merriweather-latin-300italic-23c3f1f88683618a4fb8d265d33d383a.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:normal;font-weight:400;src:local("Merriweather Regular "),local("Merriweather-Regular"),url(/static/merriweather-latin-400-d9479e8023bef9cbd9bf8d6eabd6bf36.woff2) format("woff2"),url(/static/merriweather-latin-400-040426f99ff6e00b86506452e0d1f10b.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:italic;font-weight:400;src:local("Merriweather Regular italic"),local("Merriweather-Regularitalic"),url(/static/merriweather-latin-400italic-2de7bfeaf08fb03d4315d49947f062f7.woff2) format("woff2"),url(/static/merriweather-latin-400italic-79db67aca65f5285964ab332bd65f451.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:normal;font-weight:700;src:local("Merriweather Bold "),local("Merriweather-Bold"),url(/static/merriweather-latin-700-4b08e01d805fa35d7bf777f1b24314ae.woff2) format("woff2"),url(/static/merriweather-latin-700-22fb8afba4ab1f093b6ef9e28a9b6e92.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:italic;font-weight:700;src:local("Merriweather Bold italic"),local("Merriweather-Bolditalic"),url(/static/merriweather-latin-700italic-cd92541b177652fffb6e3b952f1c33f1.woff2) format("woff2"),url(/static/merriweather-latin-700italic-f87f3d87cea0dd0979bfc8ac9ea90243.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:normal;font-weight:900;src:local("Merriweather Black "),local("Merriweather-Black"),url(/static/merriweather-latin-900-f813fc6a4bee46eda5224ac7ebf1b7be.woff2) format("woff2"),url(/static/merriweather-latin-900-5d4e42cb44410674acd99153d57df032.woff) format("woff")}@font-face{font-display:swap;font-family:Merriweather;font-style:italic;font-weight:900;src:local("Merriweather Black italic"),local("Merriweather-Blackitalic"),url(/static/merriweather-latin-900italic-b7901d85486871c1779c0e93ddd85656.woff2) format("woff2"),url(/static/merriweather-latin-900italic-9647f9fdab98756989a8a5550eb205c3.woff) format("woff")}


/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{-webkit-text-size-adjust:100%;line-height:1.15}body{margin:0}main{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details{display:block}summary{display:list-item}[hidden]{display:none}:root{--maxWidth-none:"none";--maxWidth-xs:20rem;--maxWidth-sm:24rem;--maxWidth-md:28rem;--maxWidth-lg:32rem;--maxWidth-xl:36rem;--maxWidth-2xl:42rem;--maxWidth-3xl:48rem;--maxWidth-4xl:56rem;--maxWidth-full:"100%";--maxWidth-wrapper:var(--maxWidth-2xl);--spacing-px:"1px";--spacing-0:0;--spacing-1:0.25rem;--spacing-2:0.5rem;--spacing-3:0.75rem;--spacing-4:1rem;--spacing-5:1.25rem;--spacing-6:1.5rem;--spacing-8:2rem;--spacing-10:2.5rem;--spacing-12:3rem;--spacing-16:4rem;--spacing-20:5rem;--spacing-24:6rem;--spacing-32:8rem;--fontFamily-sans:Montserrat,system-ui,-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,"Noto Sans",sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol","Noto Color Emoji";--fontFamily-serif:"Merriweather","Georgia",Cambria,"Times New Roman",Times,serif;--font-body:var(--fontFamily-serif);--font-heading:var(--fontFamily-sans);--fontWeight-normal:400;--fontWeight-medium:500;--fontWeight-semibold:600;--fontWeight-bold:700;--fontWeight-extrabold:800;--fontWeight-black:900;--fontSize-root:16px;--lineHeight-none:1;--lineHeight-tight:1.1;--lineHeight-normal:1.5;--lineHeight-relaxed:1.625;--fontSize-0:0.833rem;--fontSize-1:1rem;--fontSize-2:1.2rem;--fontSize-3:1.44rem;--fontSize-4:1.728rem;--fontSize-5:2.074rem;--fontSize-6:2.488rem;--fontSize-7:2.986rem;--color-primary:#005b99;--color-text:#2e353f;--color-text-light:#4f5969;--color-heading:#1a202c;--color-heading-black:#000;--color-accent:#d1dce5}*,:after,:before{box-sizing:border-box}html{-webkit-font-smoothing:antialiased;-moz-osx-font-smoothing:grayscale;font-size:var(--fontSize-root);line-height:var(--lineHeight-normal)}body{color:var(--color-text);font-family:var(--font-body);font-size:var(--fontSize-1)}footer{padding:var(--spacing-6) var(--spacing-0)}hr{background:var(--color-accent);border:0;height:1px}h1,h2,h3,h4,h5,h6{font-family:var(--font-heading);letter-spacing:-.025em;line-height:var(--lineHeight-tight);margin-bottom:var(--spacing-6);margin-top:var(--spacing-12)}h2,h3,h4,h5,h6{color:var(--color-heading);font-weight:var(--fontWeight-bold)}h1{color:var(--color-heading-black);font-size:var(--fontSize-6);font-weight:var(--fontWeight-black)}h2{font-size:var(--fontSize-5)}h3{font-size:var(--fontSize-4)}h4{font-size:var(--fontSize-3)}h5{font-size:var(--fontSize-2)}h6{font-size:var(--fontSize-1)}h1>a,h2>a,h3>a,h4>a,h5>a,h6>a{color:inherit;text-decoration:none}p{--baseline-multiplier:0.179;--x-height-multiplier:0.35;line-height:var(--lineHeight-relaxed);margin:var(--spacing-0) var(--spacing-0) var(--spacing-8) var(--spacing-0)}ol,p,ul{padding:var(--spacing-0)}ol,ul{list-style-image:none;list-style-position:outside;margin-bottom:var(--spacing-8);margin-left:var(--spacing-0);margin-right:var(--spacing-0)}ol li,ul li{padding-left:var(--spacing-0)}li>p,ol li,ul li{margin-bottom:calc(var(--spacing-8)/2)}li :last-child{margin-bottom:var(--spacing-0)}li>ul{margin-left:var(--spacing-8);margin-top:calc(var(--spacing-8)/2)}blockquote{border-left:var(--spacing-1) solid var(--color-primary);color:var(--color-text-light);font-size:var(--fontSize-2);font-style:italic;margin-bottom:var(--spacing-8);margin-left:calc(var(--spacing-6)*-1);margin-right:var(--spacing-8);padding:var(--spacing-0) var(--spacing-0) var(--spacing-0) var(--spacing-6)}blockquote>:last-child{margin-bottom:var(--spacing-0)}blockquote>ol,blockquote>ul{list-style-position:inside}table{border-collapse:collapse;border-spacing:.25rem;margin-bottom:var(--spacing-8);width:100%}table thead tr th{border-bottom:1px solid var(--color-accent)}a{color:var(--color-primary)}a:focus,a:hover{text-decoration:none}.global-wrapper{margin:var(--spacing-0) auto;max-width:var(--maxWidth-wrapper);padding:var(--spacing-10) var(--spacing-5)}.global-wrapper[data-is-root-path=true] .bio{margin-bottom:var(--spacing-20)}.global-header{margin-bottom:var(--spacing-12)}.main-heading{font-size:var(--fontSize-7);margin:0}.post-list-item{margin-bottom:var(--spacing-8);margin-top:var(--spacing-8)}.post-list-item p{margin-bottom:var(--spacing-0)}.post-list-item h2{color:var(--color-primary);font-size:var(--fontSize-4);margin-bottom:var(--spacing-2);margin-top:var(--spacing-0)}.post-list-item header{margin-bottom:var(--spacing-4)}.header-link-home{font-family:var(--font-heading);font-size:var(--fontSize-2);font-weight:var(--fontWeight-bold);text-decoration:none}.bio{display:flex;margin-bottom:var(--spacing-16)}.bio p,.bio-avatar{margin-bottom:var(--spacing-0)}.bio-avatar{border-radius:100%;margin-right:var(--spacing-4);min-width:50px}.blog-post header h1{margin:var(--spacing-0) var(--spacing-0) var(--spacing-4) var(--spacing-0)}.blog-post header p{font-family:var(--font-heading);font-size:var(--fontSize-2)}.blog-post-nav ul{margin:var(--spacing-0)}.gatsby-highlight{margin-bottom:var(--spacing-8)}@media (max-width:42rem){blockquote{margin-left:var(--spacing-0);padding:var(--spacing-0) var(--spacing-0) var(--spacing-0) var(--spacing-4)}ol,ul{list-style-position:inside}}code[class*=language-],pre[class*=language-]{word-wrap:normal;background:none;color:#000;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:1em;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none;line-height:1.5;-o-tab-size:4;tab-size:4;text-align:left;text-shadow:0 1px #fff;white-space:pre;word-break:normal;word-spacing:normal}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{background:#b3d4fc;text-shadow:none}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}pre[class*=language-]{margin:.5em 0;overflow:auto;padding:1em}:not(pre)>code[class*=language-],pre[class*=language-]{background:#f5f2f0}:not(pre)>code[class*=language-]{border-radius:.3em;padding:.1em;white-space:normal}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#708090}.token.punctuation{color:#999}.token.namespace{opacity:.7}.token.boolean,.token.constant,.token.deleted,.token.number,.token.property,.token.symbol,.token.tag{color:#905}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#690}.language-css .token.string,.style .token.string,.token.entity,.token.operator,.token.url{background:hsla(0,0%,100%,.5);color:#9a6e3a}.token.atrule,.token.attr-value,.token.keyword{color:#07a}.token.class-name,.token.function{color:#dd4a68}.token.important,.token.regex,.token.variable{color:#e90}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}</style><style>.gatsby-image-wrapper{position:relative;overflow:hidden}.gatsby-image-wrapper picture.object-fit-polyfill{position:static!important}.gatsby-image-wrapper img{bottom:0;height:100%;left:0;margin:0;max-width:none;padding:0;position:absolute;right:0;top:0;width:100%;object-fit:cover}.gatsby-image-wrapper [data-main-image]{opacity:0;transform:translateZ(0);transition:opacity .25s linear;will-change:opacity}.gatsby-image-wrapper-constrained{display:inline-block;vertical-align:top}</style><noscript><style>.gatsby-image-wrapper noscript [data-main-image]{opacity:1!important}.gatsby-image-wrapper [data-placeholder-image]{opacity:0!important}</style></noscript><script type="module">const e="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;e&&document.body.addEventListener("load",(function(e){if(void 0===e.target.dataset.mainImage)return;if(void 0===e.target.dataset.gatsbyImageSsr)return;const t=e.target;let a=null,n=t;for(;null===a&&n;)void 0!==n.parentNode.dataset.gatsbyImageWrapper&&(a=n.parentNode),n=n.parentNode;const o=a.querySelector("[data-placeholder-image]"),r=new Image;r.src=t.currentSrc,r.decode().catch((()=>{})).then((()=>{t.style.opacity=1,o&&(o.style.opacity=0,o.style.transition="opacity 500ms linear")}))}),!0);</script><link rel="alternate" type="application/rss+xml" title="Gatsby Starter Blog RSS Feed" href="/rss.xml"/><link rel="icon" href="/favicon-32x32.png?v=e8d56e807ace6dccd6cb4fe2ee4f7ab7" type="image/png"/><link rel="manifest" href="/manifest.webmanifest" crossorigin="anonymous"/><link rel="apple-touch-icon" sizes="48x48" href="/icons/icon-48x48.png?v=e8d56e807ace6dccd6cb4fe2ee4f7ab7"/><link rel="apple-touch-icon" sizes="72x72" href="/icons/icon-72x72.png?v=e8d56e807ace6dccd6cb4fe2ee4f7ab7"/><link rel="apple-touch-icon" sizes="96x96" href="/icons/icon-96x96.png?v=e8d56e807ace6dccd6cb4fe2ee4f7ab7"/><link rel="apple-touch-icon" sizes="144x144" href="/icons/icon-144x144.png?v=e8d56e807ace6dccd6cb4fe2ee4f7ab7"/><link rel="apple-touch-icon" sizes="192x192" href="/icons/icon-192x192.png?v=e8d56e807ace6dccd6cb4fe2ee4f7ab7"/><link rel="apple-touch-icon" sizes="256x256" href="/icons/icon-256x256.png?v=e8d56e807ace6dccd6cb4fe2ee4f7ab7"/><link rel="apple-touch-icon" sizes="384x384" href="/icons/icon-384x384.png?v=e8d56e807ace6dccd6cb4fe2ee4f7ab7"/><link rel="apple-touch-icon" sizes="512x512" href="/icons/icon-512x512.png?v=e8d56e807ace6dccd6cb4fe2ee4f7ab7"/><title data-react-helmet="true">Conduct Legal Research with AI: Part 3 | Justin&#x27;s Data Blog</title><link as="script" rel="preload" href="/webpack-runtime-8d4d3bcfd5afb11d1043.js"/><link as="script" rel="preload" href="/framework-e413e527015be9a1bdfd.js"/><link as="script" rel="preload" href="/app-c8d1e6b1e07552f29524.js"/><link as="script" rel="preload" href="/commons-9979cba14fa891b0ecd4.js"/><link as="script" rel="preload" href="/component---src-templates-blog-post-js-2fd61fd1c97edc75b172.js"/><link as="fetch" rel="preload" href="/page-data/ai-proof-of-concept/page-data.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/2841359383.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/sq/d/3257411868.json" crossorigin="anonymous"/><link as="fetch" rel="preload" href="/page-data/app-data.json" crossorigin="anonymous"/></head><body><div id="___gatsby"><div style="outline:none" tabindex="-1" id="gatsby-focus-wrapper"><div class="global-wrapper" data-is-root-path="false"><header class="global-header"><a class="header-link-home" href="/">Justin&#x27;s Data Blog</a></header><main><article class="blog-post" itemscope="" itemType="http://schema.org/Article"><header><h1 itemProp="headline">Conduct Legal Research with AI: Part 3</h1><div data-gatsby-image-wrapper="" class="gatsby-image-wrapper"><div aria-hidden="true" style="padding-top:50%"></div><img aria-hidden="true" data-placeholder-image="" style="opacity:1;transition:opacity 500ms linear" decoding="async" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAIAAAA7N+mxAAAACXBIWXMAAAsTAAALEwEAmpwYAAAA+0lEQVQoz2P4jwO8f/X13bOPv///e/H5Ay41DJhC//7+O77k5MyotsWxBVvbmzIb8lcc3UGs5r9//i0rXDbLyH6uutpMVfVaV7ukyTU///wGmUtQ859ff1fWrp5ubLdET2+GlnaDi33a5Pofv38Rpfnf33+fXn862NaxyMRogrHJ8Wkzbj29//P3b8LO/vcPZPr3Hz/jE1tcdD0sdJxKSycTG2AQze/efjQ1DtTSiFTTdLa3D/327SdcirDmt28+GBp5aKl7q6gbO7mE/PjxmwTNP77/io8rkZezV1E3S04u/f37L1iKiKiCgF8/f29Ye+DQwTO/fv3G5WcAwE4LnUVWCOEAAAAASUVORK5CYII=" alt=""/><picture><source type="image/avif" data-srcset="/static/515df4a41de5943f02d20899cf6a3b71/06049/post-image.avif 750w,/static/515df4a41de5943f02d20899cf6a3b71/0f115/post-image.avif 1080w,/static/515df4a41de5943f02d20899cf6a3b71/5b80a/post-image.avif 1200w" sizes="100vw"/><source type="image/webp" data-srcset="/static/515df4a41de5943f02d20899cf6a3b71/ee7ce/post-image.webp 750w,/static/515df4a41de5943f02d20899cf6a3b71/819dc/post-image.webp 1080w,/static/515df4a41de5943f02d20899cf6a3b71/f9756/post-image.webp 1200w" sizes="100vw"/><img data-gatsby-image-ssr="" data-main-image="" style="opacity:0" sizes="100vw" decoding="async" loading="lazy" data-src="/static/515df4a41de5943f02d20899cf6a3b71/a6312/post-image.png" data-srcset="/static/515df4a41de5943f02d20899cf6a3b71/e7dcc/post-image.png 750w,/static/515df4a41de5943f02d20899cf6a3b71/50eb2/post-image.png 1080w,/static/515df4a41de5943f02d20899cf6a3b71/a6312/post-image.png 1200w" alt="Justin Napolitano"/></picture><noscript><picture><source type="image/avif" srcSet="/static/515df4a41de5943f02d20899cf6a3b71/06049/post-image.avif 750w,/static/515df4a41de5943f02d20899cf6a3b71/0f115/post-image.avif 1080w,/static/515df4a41de5943f02d20899cf6a3b71/5b80a/post-image.avif 1200w" sizes="100vw"/><source type="image/webp" srcSet="/static/515df4a41de5943f02d20899cf6a3b71/ee7ce/post-image.webp 750w,/static/515df4a41de5943f02d20899cf6a3b71/819dc/post-image.webp 1080w,/static/515df4a41de5943f02d20899cf6a3b71/f9756/post-image.webp 1200w" sizes="100vw"/><img data-gatsby-image-ssr="" data-main-image="" style="opacity:0" sizes="100vw" decoding="async" loading="lazy" src="/static/515df4a41de5943f02d20899cf6a3b71/a6312/post-image.png" srcSet="/static/515df4a41de5943f02d20899cf6a3b71/e7dcc/post-image.png 750w,/static/515df4a41de5943f02d20899cf6a3b71/50eb2/post-image.png 1080w,/static/515df4a41de5943f02d20899cf6a3b71/a6312/post-image.png 1200w" alt="Justin Napolitano"/></picture></noscript><script type="module">const t="undefined"!=typeof HTMLImageElement&&"loading"in HTMLImageElement.prototype;if(t){const t=document.querySelectorAll("img[data-main-image]");for(let e of t){e.dataset.src&&(e.setAttribute("src",e.dataset.src),e.removeAttribute("data-src")),e.dataset.srcset&&(e.setAttribute("srcset",e.dataset.srcset),e.removeAttribute("data-srcset"));const t=e.parentNode.querySelectorAll("source[data-srcset]");for(let e of t)e.setAttribute("srcset",e.dataset.srcset),e.removeAttribute("data-srcset");e.complete&&(e.style.opacity=1)}}</script></div><p>May 17, 2022</p><p>Predicting the outcomes of Supreme Court Cases with TensorFlow</p><p>Justin Napolitano</p></header><section itemProp="articleBody"><h1>Conduct Legal Research with AI: Part 3</h1>
<p>This is the third post in a series documenting the process of building an ml pipeline that will be used to train models to predict the outcomes of Supreme Court Cases.</p>
<p>You can find the others at:</p>
<ul>
<li><a href="https://blog.jnapolitano.io/neo4j_integration/">blog.jnapolitano.io/neo4j_integration/</a></li>
<li><a href="https://blog.jnapolitano.io/constitution_to_neo/">blog.jnapolitano.io/constitution_to_neo/</a></li>
</ul>
<h2>Introduction</h2>
<p>In this post, I will be testing a sample TensorFlow pipeline against the <a href="http://scdb.wustl.edu/">Supreme Court Database</a> maintained by the Washington University Law School in order to build a proof of concept model for a Supreme Court Graph Analysis project.</p>
<h2>The Data Set</h2>
<p>The data set is fairly limited. There are only about 250 issue areas to train against. It also does not include doctrinal trees to train an algorithm to predict when the court will break from precedent.</p>
<p>The data records primarily the voting behavior of justices, the policy direction of the outcome, and the political ideology score of the outcome.</p>
<p>Policy direction and political ideology are factors coded by individuals that may be prone to error an bias. In my analysis I will test against the findings of this model to determine which factors are most powerful.</p>
<h2>The Model</h2>
<p>I tested against case centric and justice centric data orientations.</p>
<h3>Case Centric Model</h3>
<p>The first is case centric.  The power of this model is fairly limited, as it attempts to discern the outcome of a case from the aggregate behavior of the current justices on the bench.  It essentially creates a score that is modified by each case outcome to determine if the court will vote conservatively or liberally towards a case of regarding a specific issue area.  It is important to note that the database limits each case to one overarching issue area.</p>
<h4>Case Centric Training Features</h4>
<ul>
<li>issue</li>
<li>issueArea</li>
<li>naturalCourt</li>
<li>petitioner</li>
<li>petitionerState</li>
<li>respondent</li>
<li>jurisdiction</li>
<li>adminAction</li>
<li>adminActionState</li>
<li>threeJudgeFdc</li>
<li>caseOrigin</li>
<li>caseOriginState</li>
<li>respondentState</li>
<li>caseSource</li>
<li>caseSourceState</li>
<li>lawType</li>
<li>lawSupp</li>
</ul>
<h4>Case Centric Results</h4>
<p>The models were able to predict the outcomes about 75 percent of the time following 1,000 epochs of training.  It is likely that factors other than previous voting behavior determine the outcome of the case.</p>
<h3>Justice Centric Model</h3>
<p>The second is justice centric.  The predictive power of this model, should in theory be greater as it attempts to determine the likely vote of an individual justice against an issue area.  Again this is limited as only the overarching issue area is available in the data.  It may very well prove that justices may favor free speech in most cases, except when the military is the target.  The model could not predict that, because the data is limited.</p>
<h4>Justice Centric Training Features</h4>
<ul>
<li>justice</li>
<li>issue</li>
<li>issueArea</li>
<li>lawType</li>
<li>direction</li>
<li>petitioner</li>
<li>naturalCourt</li>
<li>petitionerState</li>
<li>respondent</li>
<li>respondentState</li>
<li>jurisdiction</li>
<li>caseOrigin</li>
<li>caseOriginState</li>
<li>caseSource</li>
<li>caseSourceState</li>
<li>certReason</li>
<li>lawSupp</li>
</ul>
<h4>Justice Centric Results</h4>
<p>The justice centric pipeline results in accuracy exceeding 82 percent.  This is better, but a model that could consider more factors and issue areas would likely improve the results.</p>
<h2>Next Steps</h2>
<p>I will integrate the models to my neo4j data set.  As of now, the graph database exceeds 50,000 individuals nodes and over 400,000 relationships.  It will allow me to train an algorithm that can determine a justices attitude towards, cases, clauses, sentences, states, petitioners, and any other feature integrated into the dataset.</p>
<h2>The Code</h2>
<h3>Case Centric Training Pipeline</h3>
<p>Iâ€™ve included the entire model below.  Review my <a href="https://docs.jnapolitano.io/parts/ml-ai/tensorflow/project-supcourt-tensorflow/docs/SupremeCourtPredictionsCase/index.html">documentation site</a> for more detail.</p>
<div class="gatsby-highlight" data-language="python"><pre class="language-python"><code class="language-python"><span class="token comment">#supremeCourtPrediction.py</span>
<span class="token comment">#a program that uses the segal and spaeth dataset to predict the outcomes of cases</span>


<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd

<span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf

<span class="token keyword">import</span> math

<span class="token keyword">import</span> seaborn <span class="token keyword">as</span> sns
<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt

<span class="token keyword">import</span> shap
shap<span class="token punctuation">.</span>initjs<span class="token punctuation">(</span><span class="token punctuation">)</span>

<span class="token keyword">import</span> os 
<span class="token keyword">import</span> csv
<span class="token keyword">import</span> datetime

<span class="token keyword">from</span> tensorflow <span class="token keyword">import</span> feature_column
<span class="token keyword">from</span> tensorflow<span class="token punctuation">.</span>keras <span class="token keyword">import</span> layers
<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split

<span class="token keyword">from</span> itertools <span class="token keyword">import</span> permutations 

<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt



<span class="token keyword">def</span> <span class="token function">get_environmental_variables</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    cwd <span class="token operator">=</span> os<span class="token punctuation">.</span>getcwd<span class="token punctuation">(</span><span class="token punctuation">)</span>
    log_dir <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>cwd<span class="token punctuation">,</span><span class="token string">"logs/fit/"</span><span class="token punctuation">)</span> <span class="token operator">+</span> datetime<span class="token punctuation">.</span>datetime<span class="token punctuation">.</span>now<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>strftime<span class="token punctuation">(</span><span class="token string">"%Y%m%d-%H%M%S"</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> cwd<span class="token punctuation">,</span> log_dir



<span class="token keyword">def</span> <span class="token function">load_data</span><span class="token punctuation">(</span>cwd<span class="token punctuation">)</span><span class="token punctuation">:</span>
    data_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>cwd<span class="token punctuation">,</span><span class="token string">'data'</span><span class="token punctuation">,</span><span class="token string">'citation.csv'</span><span class="token punctuation">)</span> 
    drop_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>cwd<span class="token punctuation">,</span><span class="token string">'data'</span><span class="token punctuation">,</span><span class="token string">'features'</span><span class="token punctuation">,</span><span class="token string">'drop'</span><span class="token punctuation">,</span><span class="token string">'drop.txt'</span><span class="token punctuation">)</span>
    <span class="token comment">#print(path)</span>
    dataframe <span class="token operator">=</span> pd<span class="token punctuation">.</span>read_csv<span class="token punctuation">(</span>data_path<span class="token punctuation">,</span>encoding<span class="token operator">=</span> <span class="token string">'unicode_escape'</span><span class="token punctuation">)</span>
    <span class="token comment">#print(dataframe.head())</span>
    
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>drop_path<span class="token punctuation">)</span> <span class="token keyword">as</span> fp<span class="token punctuation">:</span>
        <span class="token keyword">for</span> cnt<span class="token punctuation">,</span> line <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>fp<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment">#print("Dropping {} from dataframe".format(line))</span>
            line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>
            line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">)</span>
            line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">"'"</span><span class="token punctuation">)</span>

            dataframe<span class="token punctuation">.</span>drop<span class="token punctuation">(</span>line<span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    
    dataframe<span class="token punctuation">.</span>fillna<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>dataframe<span class="token punctuation">)</span>
    <span class="token comment">#dataframe.issue = dataframe.astype({'issue': 'str'})</span>
    <span class="token comment">#dataframe[issue] = dataframe[issue].astype(str)</span>
    <span class="token comment">#print(dataframe)</span>
    <span class="token comment">#print(dataframe.head())</span>
    <span class="token keyword">return</span> dataframe



<span class="token keyword">def</span> <span class="token function">split_datframe</span><span class="token punctuation">(</span>dataframe<span class="token punctuation">)</span><span class="token punctuation">:</span>
    train<span class="token punctuation">,</span> test <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>dataframe<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
    train<span class="token punctuation">,</span> val <span class="token operator">=</span> train_test_split<span class="token punctuation">(</span>train<span class="token punctuation">,</span> test_size<span class="token operator">=</span><span class="token number">0.2</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>train<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'train examples'</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>val<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'validation examples'</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">len</span><span class="token punctuation">(</span>test<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token string">'test examples'</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> train<span class="token punctuation">,</span> test<span class="token punctuation">,</span> val


    <span class="token comment"># A utility method to create a tf.data dataset from a Pandas Dataframe</span>

<span class="token keyword">def</span> <span class="token function">df_to_dataset</span><span class="token punctuation">(</span>dataframe<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    dataframe <span class="token operator">=</span> dataframe<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
    labels <span class="token operator">=</span> dataframe<span class="token punctuation">.</span>pop<span class="token punctuation">(</span><span class="token string">'declarationUncon'</span><span class="token punctuation">)</span>
    ds <span class="token operator">=</span> tf<span class="token punctuation">.</span>data<span class="token punctuation">.</span>Dataset<span class="token punctuation">.</span>from_tensor_slices<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token builtin">dict</span><span class="token punctuation">(</span>dataframe<span class="token punctuation">)</span><span class="token punctuation">,</span> labels<span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">if</span> shuffle<span class="token punctuation">:</span>
        ds <span class="token operator">=</span> ds<span class="token punctuation">.</span>shuffle<span class="token punctuation">(</span>buffer_size<span class="token operator">=</span><span class="token builtin">len</span><span class="token punctuation">(</span>dataframe<span class="token punctuation">)</span><span class="token punctuation">)</span>
    ds <span class="token operator">=</span> ds<span class="token punctuation">.</span>batch<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>ds<span class="token punctuation">)</span>
    <span class="token keyword">return</span> ds



<span class="token keyword">def</span> <span class="token function">get_input_pipeline</span><span class="token punctuation">(</span>train<span class="token punctuation">,</span> test<span class="token punctuation">,</span> val<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    train_ds <span class="token operator">=</span> df_to_dataset<span class="token punctuation">(</span>train<span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>
    val_ds <span class="token operator">=</span> df_to_dataset<span class="token punctuation">(</span>val<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>
    test_ds <span class="token operator">=</span> df_to_dataset<span class="token punctuation">(</span>test<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span>batch_size<span class="token punctuation">)</span>
    <span class="token keyword">return</span> train_ds<span class="token punctuation">,</span> val_ds<span class="token punctuation">,</span> test_ds



<span class="token keyword">def</span> <span class="token function">get_feature_layer</span><span class="token punctuation">(</span>cwd<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment">#select the columns for analysis from dataset</span>
    feature_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>cwd<span class="token punctuation">,</span><span class="token string">'data'</span><span class="token punctuation">,</span><span class="token string">'features'</span><span class="token punctuation">,</span><span class="token string">'use'</span><span class="token punctuation">,</span><span class="token string">'features.txt'</span><span class="token punctuation">)</span>
    dict_path <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>cwd<span class="token punctuation">,</span><span class="token string">'data'</span><span class="token punctuation">,</span><span class="token string">'features'</span><span class="token punctuation">,</span><span class="token string">'use'</span><span class="token punctuation">)</span>
    feature_columns <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>
    
    <span class="token comment">#embedded columns</span>
    <span class="token keyword">with</span> <span class="token builtin">open</span><span class="token punctuation">(</span>feature_path<span class="token punctuation">)</span> <span class="token keyword">as</span> fp<span class="token punctuation">:</span>
        <span class="token keyword">for</span> cnt<span class="token punctuation">,</span> line <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>fp<span class="token punctuation">)</span><span class="token punctuation">:</span>
            <span class="token comment">#print("Adding {} to features".format(line))</span>
            line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'\n'</span><span class="token punctuation">)</span>
            line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">'\t'</span><span class="token punctuation">)</span>
            line <span class="token operator">=</span> line<span class="token punctuation">.</span>strip<span class="token punctuation">(</span><span class="token string">"'"</span><span class="token punctuation">)</span>

            <span class="token keyword">print</span><span class="token punctuation">(</span>cnt<span class="token punctuation">)</span>   
            <span class="token keyword">print</span><span class="token punctuation">(</span>line<span class="token punctuation">)</span>

            indicator <span class="token operator">=</span> feature_column<span class="token punctuation">.</span>numeric_column<span class="token punctuation">(</span>line<span class="token punctuation">)</span>
            
            <span class="token comment">#feature_column.categorical_column_with_vocabulary_file(</span>
            <span class="token comment">#            key = line, vocabulary_file = os.path.join(dict_path,"{}.txt".format(line)), default_value=0)</span>

            <span class="token keyword">print</span><span class="token punctuation">(</span>indicator<span class="token punctuation">)</span>
            feature_columns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>indicator<span class="token punctuation">)</span>


            <span class="token comment">#feature_columns.append(feature_column.embedding_column(indicator, dimension=8))</span>

    
    crossed_feature <span class="token operator">=</span> feature_column<span class="token punctuation">.</span>crossed_column<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'issue'</span><span class="token punctuation">,</span> <span class="token string">'issueArea'</span><span class="token punctuation">,</span> <span class="token string">'naturalCourt'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hash_bucket_size<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>
    crossed_feature <span class="token operator">=</span> feature_column<span class="token punctuation">.</span>indicator_column<span class="token punctuation">(</span>crossed_feature<span class="token punctuation">)</span>
    feature_columns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>crossed_feature<span class="token punctuation">)</span>


    crossed_feature <span class="token operator">=</span> feature_column<span class="token punctuation">.</span>crossed_column<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'issue'</span><span class="token punctuation">,</span> <span class="token string">'naturalCourt'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hash_bucket_size<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>
    crossed_feature <span class="token operator">=</span> feature_column<span class="token punctuation">.</span>indicator_column<span class="token punctuation">(</span>crossed_feature<span class="token punctuation">)</span>
    feature_columns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>crossed_feature<span class="token punctuation">)</span>

    crossed_feature <span class="token operator">=</span> feature_column<span class="token punctuation">.</span>crossed_column<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token string">'issueArea'</span><span class="token punctuation">,</span> <span class="token string">'naturalCourt'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>hash_bucket_size<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span>
    crossed_feature <span class="token operator">=</span> feature_column<span class="token punctuation">.</span>indicator_column<span class="token punctuation">(</span>crossed_feature<span class="token punctuation">)</span>
    feature_columns<span class="token punctuation">.</span>append<span class="token punctuation">(</span>crossed_feature<span class="token punctuation">)</span>
    
   <span class="token comment"># court_buckets = feature_column.bucketized_column(naturalCourt, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])</span>

    <span class="token comment">#print(feature_columns)</span>
    feature_layer <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>DenseFeatures<span class="token punctuation">(</span>feature_columns<span class="token punctuation">)</span>
    <span class="token comment">#for item in feature_columns:</span>
    <span class="token comment">#    print(item)</span>
    <span class="token comment">#    print("\n")</span>

    <span class="token keyword">return</span> feature_layer

     
    <span class="token comment">#crossed cols</span>


    <span class="token comment">#categorical_columns </span>

<span class="token keyword">def</span> <span class="token function">understand_input_pipeline</span><span class="token punctuation">(</span>train_ds<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">for</span> feature_batch<span class="token punctuation">,</span> label_batch <span class="token keyword">in</span> train_ds<span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Every feature:'</span><span class="token punctuation">,</span> <span class="token builtin">list</span><span class="token punctuation">(</span>feature_batch<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
        <span class="token comment">#print('A batch of ages:', feature_batch['age'])</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'A batch of targets:'</span><span class="token punctuation">,</span> label_batch <span class="token punctuation">)</span>



<span class="token keyword">def</span> <span class="token function">create_model</span><span class="token punctuation">(</span>log_dir<span class="token punctuation">,</span>feature_layer<span class="token punctuation">,</span> train_ds<span class="token punctuation">,</span> val_ds<span class="token punctuation">,</span> epochs <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    model <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">[</span>
        feature_layer<span class="token punctuation">,</span>
        layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> activation<span class="token operator">=</span><span class="token string">'relu'</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
        layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>
    <span class="token punctuation">]</span><span class="token punctuation">)</span>

    model<span class="token punctuation">.</span><span class="token builtin">compile</span><span class="token punctuation">(</span>optimizer<span class="token operator">=</span><span class="token string">'adam'</span><span class="token punctuation">,</span>
            loss<span class="token operator">=</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>BinaryCrossentropy<span class="token punctuation">(</span>from_logits<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
            metrics<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'accuracy'</span><span class="token punctuation">,</span> <span class="token string">'mae'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

    history <span class="token operator">=</span> model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>train_ds<span class="token punctuation">,</span>
            validation_data<span class="token operator">=</span>val_ds<span class="token punctuation">,</span>
            epochs<span class="token operator">=</span>epochs<span class="token punctuation">)</span>

    loss<span class="token punctuation">,</span> accuracy<span class="token punctuation">,</span>mae <span class="token operator">=</span> model<span class="token punctuation">.</span>evaluate<span class="token punctuation">(</span>test_ds<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Accuracy"</span><span class="token punctuation">,</span> accuracy<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>model<span class="token punctuation">.</span>summary<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">return</span> model<span class="token punctuation">,</span>history


<span class="token keyword">def</span> <span class="token function">plot_history</span><span class="token punctuation">(</span>history<span class="token punctuation">)</span><span class="token punctuation">:</span>

    hist <span class="token operator">=</span> pd<span class="token punctuation">.</span>DataFrame<span class="token punctuation">(</span>history<span class="token punctuation">.</span>history<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>hist<span class="token punctuation">)</span>
    hist<span class="token punctuation">[</span><span class="token string">'epoch'</span><span class="token punctuation">]</span> <span class="token operator">=</span> history<span class="token punctuation">.</span>epoch

    plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>xlabel<span class="token punctuation">(</span><span class="token string">'Epoch'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>ylabel<span class="token punctuation">(</span><span class="token string">'Mean Absolute Error'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>hist<span class="token punctuation">[</span><span class="token string">'epoch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hist<span class="token punctuation">[</span><span class="token string">'mae'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            label<span class="token operator">=</span><span class="token string">'Train Error'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>hist<span class="token punctuation">[</span><span class="token string">'epoch'</span><span class="token punctuation">]</span><span class="token punctuation">,</span> hist<span class="token punctuation">[</span><span class="token string">'val_mae'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
            label <span class="token operator">=</span> <span class="token string">'Val Error'</span><span class="token punctuation">)</span>
    plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>



<span class="token keyword">def</span> <span class="token function">explain_kernal</span><span class="token punctuation">(</span>df_train<span class="token punctuation">,</span>model<span class="token punctuation">,</span>train_ds<span class="token punctuation">)</span><span class="token punctuation">:</span>
    
    <span class="token comment"># KernelExplainer is a general approach that can work with any ML framework</span>
    <span class="token comment"># Its inputs are the predictions and training data</span>

    <span class="token comment"># Summarize the training set to accelerate analysis</span>
    df_train_summary <span class="token operator">=</span> shap<span class="token punctuation">.</span>kmeans<span class="token punctuation">(</span>df_train<span class="token punctuation">.</span>values<span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">)</span>

    <span class="token comment"># Instantiate an explainer with the model predictions and training data summary</span>
    explainer <span class="token operator">=</span> shap<span class="token punctuation">.</span>KernelExplainer<span class="token punctuation">(</span>model<span class="token punctuation">.</span>predict<span class="token punctuation">,</span> df_train<span class="token punctuation">)</span>


<span class="token comment"># Extract Shapley values from the explainer</span>
    <span class="token comment">#shap_values = explainer.shap_values(df_train.values)</span>
<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>
    cwd<span class="token punctuation">,</span> log_dir <span class="token operator">=</span> get_environmental_variables<span class="token punctuation">(</span><span class="token punctuation">)</span>
    <span class="token comment">#print(cwd)</span>

    df <span class="token operator">=</span> load_data<span class="token punctuation">(</span>cwd<span class="token punctuation">)</span>

    train<span class="token punctuation">,</span>test<span class="token punctuation">,</span>val <span class="token operator">=</span> split_datframe<span class="token punctuation">(</span>df<span class="token punctuation">)</span>
    dataset <span class="token operator">=</span> df_to_dataset<span class="token punctuation">(</span>df<span class="token punctuation">)</span>
    train_ds<span class="token punctuation">,</span> val_ds<span class="token punctuation">,</span> test_ds <span class="token operator">=</span> get_input_pipeline<span class="token punctuation">(</span>train<span class="token punctuation">,</span>test<span class="token punctuation">,</span>val<span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span>
    understand_input_pipeline<span class="token punctuation">(</span>train_ds<span class="token punctuation">)</span>
    feature_layer <span class="token operator">=</span> get_feature_layer<span class="token punctuation">(</span>cwd<span class="token punctuation">)</span>
    model<span class="token punctuation">,</span> model_history <span class="token operator">=</span> create_model<span class="token punctuation">(</span>log_dir<span class="token punctuation">,</span>feature_layer<span class="token punctuation">,</span> train_ds<span class="token punctuation">,</span> val_ds<span class="token punctuation">)</span>
    plot_history<span class="token punctuation">(</span>model_history<span class="token punctuation">)</span>
    <span class="token comment">#print(model.predict(train_ds))</span>
    <span class="token comment">#print(model.predict)</span>
    explain_kernal<span class="token punctuation">(</span>df<span class="token punctuation">,</span>model<span class="token punctuation">,</span>train_ds<span class="token punctuation">)</span></code></pre></div>
<h3>The Justice Centric Training Pipeline</h3>
<p>The entire pipeline is included below.  Review the <a href="https://docs.jnapolitano.io/parts/ml-ai/tensorflow/project-supcourt-tensorflow/docs/SupremeCourtPredictionsJustice/index.html">documentation</a> for greater detail.</p>
<div class="gatsby-highlight" data-language="text"><pre class="language-text"><code class="language-text">#supremeCourtPrediction.py
#a program that uses the segal and spaeth dataset to predict the outcomes of cases


import pandas as pd

import tensorflow as tf

import math

import seaborn as sns
import matplotlib.pyplot as plt

import shap
shap.initjs()

import os 
import csv
import datetime

from tensorflow import feature_column
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

from itertools import permutations
from itertools import combinations_with_replacement

import matplotlib.pyplot as plt




def get_environmental_variables():
    cwd = os.getcwd()
    log_dir = os.path.join(cwd,"logs/fit/") + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    return cwd, log_dir




def load_data(cwd):
    data_path = os.path.join(cwd,'data','justice.csv') 
    drop_path = os.path.join(cwd,'data','features','drop','drop.txt')
    target_path = os.path.join(cwd,'data','features','targets.txt')
    print(data_path)
    dataframe = pd.read_csv(data_path,encoding= 'unicode_escape')
    #print(dataframe.head())
    
    with open(drop_path) as fp:
        for cnt, line in enumerate(fp):
            #print("Dropping {} from dataframe".format(line))
            line = line.strip('\n')
            line = line.strip('\t')
            line = line.strip("'")

            dataframe.drop(line,axis=1,inplace=True)
    
    dataframe.fillna(0, inplace=True)
    print(dataframe)
    #dataframe.issue = dataframe.astype({'issue': 'str'})
    #dataframe[issue] = dataframe[issue].astype(str)
    #print(dataframe)
    #print(dataframe.head())
    return dataframe




def split_datframe(dataframe):
    train, test = train_test_split(dataframe, test_size=0.2)
    train, val = train_test_split(train, test_size=0.2)
    print(len(train), 'train examples')
    print(len(val), 'validation examples')
    print(len(test), 'test examples')
    return train, test, val


    # A utility method to create a tf.data dataset from a Pandas Dataframe


def df_to_dataset(dataframe, shuffle=True, batch_size=32):
    dataframe = dataframe.copy()
    labels = dataframe.pop('vote')
    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
    if shuffle:
        ds = ds.shuffle(buffer_size=len(dataframe))
    ds = ds.batch(batch_size)
    print(ds)
    return ds




def get_input_pipeline(train, test, val, batch_size=32, shuffle=True):
    train_ds = df_to_dataset(train, batch_size=batch_size)
    val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)
    test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)
    return train_ds, val_ds, test_ds




def get_feature_layer(cwd): #select the columns for analysis from dataset
    feature_path = os.path.join(cwd,'data','features','use','features.txt')
    dict_path = os.path.join(cwd,'data','features','use')
    feature_columns = []
    feature_list = []
    #embedded columns
    with open(feature_path) as fp:
        for cnt, line in enumerate(fp):
            #print("Adding {} to features".format(line))
            line = line.strip('\n')
            line = line.strip('\t')
            line = line.strip("'")

            print(cnt)   
            print(line)
            feature_list.append(line)

            indicator = feature_column.numeric_column(line)
            
            #feature_column.categorical_column_with_vocabulary_file(
            #            key = line, vocabulary_file = os.path.join(dict_path,"{}.txt".format(line)), default_value=0)

            print(indicator)
            feature_columns.append(indicator)


            #feature_columns.append(feature_column.embedding_column(indicator, dimension=8))

    feature_comb = combinations_with_replacement(feature_list, 2) 
    lst = list(feature_comb)
    limit = len(feature_list)
    print(limit)
    j = 0
    k = limit - 1
    lookup = []
    for i in range(limit):
        lookup.append(i+j)
        j = j + k
        k = k - 1 


    for i in range(len(lst)):
        if i in lookup:
            continue
        else:
            one, two = lst[i]
            crossed_feature = feature_column.crossed_column([one, two], hash_bucket_size=1000)
            crossed_feature = feature_column.indicator_column(crossed_feature)
            feature_columns.append(crossed_feature)

    """
    crossed_feature = feature_column.crossed_column(['issue', 'naturalCourt'], hash_bucket_size=1000)
    crossed_feature = feature_column.indicator_column(crossed_feature)
    feature_columns.append(crossed_feature)

    crossed_feature = feature_column.crossed_column(['issueArea', 'naturalCourt'],hash_bucket_size=1000)
    crossed_feature = feature_column.indicator_column(crossed_feature)
    feature_columns.append(crossed_feature)
    """
   # court_buckets = feature_column.bucketized_column(naturalCourt, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])

    #print(feature_columns)
    feature_layer = tf.keras.layers.DenseFeatures(feature_columns)
    #for item in feature_columns:
    #    print(item)
    #    print("\n")

    return feature_layer

     
    #crossed cols


    #categorical_columns 


def understand_input_pipeline(train_ds):
    for feature_batch, label_batch in train_ds.take(1):
        print('Every feature:', list(feature_batch.keys()))
        #print('A batch of ages:', feature_batch['age'])
        print('A batch of targets:', label_batch )




def create_model(log_dir,feature_layer, train_ds, val_ds, epochs = 8):
    
    model = tf.keras.Sequential([
        feature_layer,
        layers.Dense(128, activation='relu'),
        layers.Dense(128, activation='relu'),
        layers.Dense(1)
    ])

    model.compile(optimizer='adam',
            loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
            metrics=['accuracy', 'mae'])

    history = model.fit(train_ds,
            validation_data=val_ds,
            epochs=epochs)

    loss, accuracy,mae = model.evaluate(test_ds)
    print("Accuracy", accuracy)
    print(model.summary())
    return model,history




def plot_history(history):

    hist = pd.DataFrame(history.history)
    print(hist)
    hist['epoch'] = history.epoch

    plt.figure()
    plt.xlabel('Epoch')
    plt.ylabel('Mean Absolute Error')
    plt.plot(hist['epoch'], hist['mae'],
            label='Train Error')
    plt.plot(hist['epoch'], hist['val_mae'],
            label = 'Val Error')
    plt.legend()




def explain_kernal(model,train_ds):
    model.predict(train_ds)
    # KernelExplainer is a general approach that can work with any ML framework
    # Its inputs are the predictions and training data

    # Summarize the training set to accelerate analysis
    df_train_summary = shap.kmeans(train_ds.values, 25)
    #print(df_train_summary)
    #model.predict(df_train_summary)
    # Instantiate an explainer with the model predictions and training data summary
    explainer = shap.KernelExplainer(model.predict, train_ds)


# Extract Shapley values from the explainer
    #shap_values = explainer.shap_values(df_train.values)
if __name__ == "__main__":
    cwd, log_dir = get_environmental_variables()
    print(cwd)


    df = load_data(cwd)

    train,test,val = split_datframe(df)
    dataset = df_to_dataset(df)
    train_ds, val_ds, test_ds = get_input_pipeline(train,test,val,32)
    understand_input_pipeline(train_ds)
    feature_layer = get_feature_layer(cwd)
    model, model_history = create_model(log_dir,feature_layer, train_ds, val_ds)
    #plot_history(model_history)
    #print(model.predict(train_ds))
    #print(model.predict)
    #explain_kernal(model,train_ds)</code></pre></div></section><hr/><footer><div class="bio"><p>Maintained by <strong>Justin Napolitano</strong>. <!-- --> <a href="https://twitter.com/just_napolitano">Follow me on Twitter</a></p></div></footer></article><nav class="blog-post-nav"><ul style="display:flex;flex-wrap:wrap;justify-content:space-between;list-style:none;padding:0"><li><a rel="prev" href="/constitution_to_neo/">â† <!-- -->Conduct Legal Research with AI: Part 2</a></li><li><a rel="next" href="/legal-research-part-4/">Conduct Legal Research with AI: Part 4<!-- --> â†’</a></li></ul></nav></main><footer>Â© <!-- -->2022<!-- -->, Created by<!-- --> <a href="https://jnapolitano.io">Justin Napolitano</a></footer></div></div><div id="gatsby-announcer" style="position:absolute;top:0;width:1px;height:1px;padding:0;overflow:hidden;clip:rect(0, 0, 0, 0);white-space:nowrap;border:0" aria-live="assertive" aria-atomic="true"></div></div><script id="gatsby-script-loader">/*<![CDATA[*/window.pagePath="/ai-proof-of-concept/";window.___webpackCompilationHash="d94363f05044c33a6781";/*]]>*/</script><script id="gatsby-chunk-mapping">/*<![CDATA[*/window.___chunkMapping={"polyfill":["/polyfill-67664a8ccc6c9f02b880.js"],"app":["/app-c8d1e6b1e07552f29524.js"],"component---src-pages-404-js":["/component---src-pages-404-js-27981ca7dd461cb4280e.js"],"component---src-pages-index-js":["/component---src-pages-index-js-59a21146d37aa180d12f.js"],"component---src-pages-using-typescript-tsx":["/component---src-pages-using-typescript-tsx-2f9b97cf148785549d09.js"],"component---src-templates-blog-post-js":["/component---src-templates-blog-post-js-2fd61fd1c97edc75b172.js"]};/*]]>*/</script><script src="/polyfill-67664a8ccc6c9f02b880.js" nomodule=""></script><script src="/component---src-templates-blog-post-js-2fd61fd1c97edc75b172.js" async=""></script><script src="/commons-9979cba14fa891b0ecd4.js" async=""></script><script src="/app-c8d1e6b1e07552f29524.js" async=""></script><script src="/framework-e413e527015be9a1bdfd.js" async=""></script><script src="/webpack-runtime-8d4d3bcfd5afb11d1043.js" async=""></script></body></html>